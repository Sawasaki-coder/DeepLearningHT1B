{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thiên vị - Phương sai (Bias - Variance Dilemma) - Tại sao chúng ta sử dụng kiểm định?\n",
    "\n",
    "Làm thế nào chúng ta có thể chắc chắn rằng thuật toán của mình thực sự đại diện cho **sự thật ngầm** $F(X)$ chứ không phải là đang **học thuộc lòng** bộ dữ liệu $y$? Chúng ta biết rằng việc *thực sự* biết $F(X)$ là điều không thể. Trong hầu hết các trường hợp, chúng ta cũng sẽ không học thuộc lòng hoàn toàn bộ dữ liệu. Hàm ước lượng $\\\\hat{F(X))}$ được tạo ra sẽ là một cái gì đó ở giữa.\n",
    "\n",
    "<img src=\"img/biasVarianceCurve.jpg\" width=\"70%\">\n",
    "\n",
    "Nguồn: https://www.quora.com/What-is-the-best-way-to-explain-the-bias-variance-trade-off-in-layman%E2%80%99s-terms\n",
    "\n",
    "Vấn đề này được gọi là **Đánh đổi Thiên vị-Phương sai (Bias-Variance tradeoff)**. Lý thuyết đằng sau nó đã được thiết lập rõ ràng và mô tả chính xác bằng toán học trong các tài liệu. Ở đây chúng ta sẽ tập trung vào trực giác.\n",
    "\n",
    "Càng cho hàm ước lượng của chúng ta nhiều **sự tự do** (độ đàn hồi/phương sai cao hơn), nó càng dễ dàng và tốt hơn trong việc khớp với dữ liệu huấn luyện. Do đó, nó sẽ đạt được **lỗi thấp hơn** (thiên vị) trên tập huấn luyện. Đồng thời, khả năng biến động càng cao thì **nguy cơ quá khớp (overfitting)** càng lớn. Mặc dù về mặt lý thuyết, chúng ta có thể giảm giá trị kỳ vọng của lỗi (thiên vị), nhưng điều này thường đi kèm với việc **tăng phương sai của lỗi**. Chúng ta có thể rơi vào tình huống mà khi dự đoán trên dữ liệu mới, lỗi rất cao do phương sai (mặc dù trung bình lỗi có thể bằng không).\n",
    "\n",
    "Để hiểu rõ hơn, hãy xem hình ảnh này:\n",
    "<img src=\"img/biasVarianceTarget.jpg\" width=\"30%\">\n",
    "\n",
    "Nguồn: https://www.quora.com/What-is-the-best-way-to-explain-the-bias-variance-trade-off-in-layman%E2%80%99s-terms\n",
    "\n",
    "Trong thực tế, phương sai cao hơn thường có nghĩa là **lỗi cao hơn** (trên dữ liệu mới). Vấn đề này được thể hiện rõ ràng trong hình minh họa sau.\n",
    "<img src=\"img/biasVarianceValid.png\" width=\"50%\">\n",
    "Nguồn: Elements of Statistical learning\n",
    "\n",
    "Đường **màu đỏ** biểu thị lỗi trên dữ liệu **\"mới\"** (kiểm định) và đường **màu xanh** biểu thị lỗi trên dữ liệu huấn luyện. Khi chúng ta nhìn vào hình ảnh này, dường như nó đã trả lời cho câu hỏi về **độ phức tạp tối ưu**. Cuối cùng, chúng ta muốn mô hình của mình hoạt động tốt nhất có thể trên dữ liệu mới chứ không phải trên dữ liệu huấn luyện. Có vẻ như việc chọn mức độ phức tạp mà tại đó đường **màu đỏ là thấp nhất** là tối ưu. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thiết lập Kiểm định Chéo như thế nào?\n",
    "Mặc dù về mặt lý thuyết, quy trình CV rất đơn giản, nhưng rất dễ mắc phải những sai lầm mà trong một số trường hợp sẽ rất tốn kém.\n",
    "\n",
    "## Phân tầng (Stratify)\n",
    "Một trong những vấn đề chúng ta có thể gặp phải là sự cân bằng của các nhãn trong tập huấn luyện và kiểm định. Điều này đặc biệt quan trọng khi chúng ta làm việc với **dữ liệu mất cân bằng (imbalanced data)**. Trong các tập dữ liệu có ít nhãn \"1\" nói chung, rất dễ hình dung một tình huống mà chúng ta rút ra không có hoặc tương đối rất ít nhãn \"1\". Nó có thể dẫn đến thiên vị và/hoặc phương sai lớn của chỉ số đánh giá trên tập dữ liệu kiểm định. Vì vấn đề này, chúng ta thường buộc phải cân bằng bằng cách sử dụng phân tầng.\n",
    "\n",
    "Ví dụ, trong trường hợp 5-fold CV, bạn có thể hình dung rằng chúng ta thực hiện việc chia tách tập huấn luyện và kiểm định riêng cho nhãn \"1\" và \"0\". Điều này đảm bảo sự cân bằng đồng đều trong cả hai nhóm.\n",
    "\n",
    "## Phân tích Chuỗi thời gian (Time series analysis)\n",
    "Tập dữ liệu khó thiết lập kiểm định chéo phù hợp nhất là phân tích chuỗi thời gian. Do thực tế là tất cả các quan sát thường được liên kết (không độc lập theo thời gian), mỗi quan sát phụ thuộc vào các giá trị trước đó. Vì đặc tính này, chúng ta không thể chỉ đơn giản là rút ngẫu nhiên các quan sát vào hai mẫu và sử dụng các hoán vị khác nhau. Các quan sát liền kề nhau về mặt thời gian chứa quá nhiều thông tin về nhau để có thể đặt chúng vào các tập khác nhau (huấn luyện và kiểm định).\n",
    "\n",
    "Một giải pháp là thực hiện việc chia tách tự nhiên dựa trên thời gian. Chúng ta sử dụng các quan sát **\"quá khứ\"** làm dữ liệu huấn luyện và các quan sát **\"tương lai\"** làm tập kiểm định. Giống như trong hình minh họa dưới đây.\n",
    "<img src=\"img/cvTimeSeries.png\" width=\"50%\">\n",
    "\n",
    "Nguồn: https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/\n",
    "\n",
    "## Rò rỉ Thông tin (Information leakage)\n",
    "Sự **độc lập** của các quan sát giữa tập huấn luyện và kiểm định là một yêu cầu quan trọng để quy trình mang lại kết quả mong đợi. Tầm quan trọng của nó được thể hiện rõ ràng trong phân tích chuỗi thời gian. Tuy nhiên, rò rỉ thông tin cũng có thể xảy ra ngay cả trong các bài toán phân loại và hồi quy thông thường, khi yêu cầu độc lập không được đáp ứng.\n",
    "\n",
    "Trong trường hợp này, khi các quan sát không độc lập, chúng ta không thể coi tập kiểm định là dữ liệu *ngoài mẫu* thực sự. Điều này dẫn đến việc **đánh giá thấp lỗi** trên tập kiểm định và có thể dẫn đến việc **đánh giá quá cao** mô hình và kết quả là một mô hình đơn giản là không hoạt động khi triển khai.\n",
    "\n",
    "Việc có nhiều quan sát cho một người trong bộ dữ liệu là một ví dụ về cấu trúc dữ liệu mà trong đó rất dễ làm mất tính độc lập của dữ liệu. Ví dụ khác có thể là việc sử dụng dữ liệu từ nhiều người trong cùng một hộ gia đình.\n",
    "\n",
    "Những lỗi trong việc thiết lập quy trình kiểm định chéo rất dễ mắc phải và khó phát hiện. Ngay cả trong các cuộc thi trên Kaggle, rò rỉ dữ liệu vẫn xảy ra theo thời gian mặc dù các tập dữ liệu được chuẩn bị bởi các chuyên gia có kinh nghiệm. Luôn cố gắng tìm hiểu dữ liệu của bạn và thiết lập CV cẩn thận.\n",
    "\n",
    "# Huấn luyện - Kiểm định - Kiểm tra (Train Validate Test)\n",
    "Tại thời điểm này, chúng ta nên chỉ ra một vấn đề quan trọng với các tuyên bố được đưa ra ở phần đầu. Chúng ta đã nói rằng chúng ta có thể sử dụng kết quả từ tập kiểm định để chọn giá trị cho các siêu tham số của thuật toán. Tuy nhiên, điều này có nghĩa là chúng ta đang chuyển thông tin từ tập kiểm định sang huấn luyện (rò rỉ dữ liệu - data leakage). Việc này, nếu bị lạm dụng, có thể dẫn đến đánh giá thiên vị và mô hình không tối ưu. Thông thường, rò rỉ thông tin là nhỏ và không đáng kể, tuy nhiên trong những trường hợp chúng ta phụ thuộc nhiều vào tập kiểm định để điều chỉnh thuật toán, chúng ta nên sử dụng quy trình huấn luyện, kiểm định, kiểm tra.\n",
    "\n",
    "Ý tưởng hoàn toàn giống như kiểm định chéo nhưng chúng ta thêm một bước nữa. Chúng ta giữ lại một phần dữ liệu (test) ở phần đầu mà chúng ta không sử dụng chút nào trong quá trình huấn luyện. Sau đó, cuối cùng, khi mọi thứ đã sẵn sàng, chúng ta kiểm tra mô hình của mình bằng tập kiểm tra này.\n",
    "<img src=\"img/cvTVT.png\">\n",
    "Nguồn: https://am207.github.io/2017/wiki/validation.html\n",
    "\n",
    "## Huấn luyện - Kiểm định - Kiểm tra trong thực tế\n",
    "Việc chia tập dữ liệu của chúng ta thành ba phần, giống như mọi thứ trong học máy, đều có cả ưu điểm và nhược điểm. Rõ ràng, lợi thế chính là có được đánh giá thuần túy và đáng tin cậy về mô hình. Chúng ta cũng không thể quên những nhược điểm:\n",
    "* Việc tách tập kiểm tra làm giảm số lượng quan sát trong tập huấn luyện của chúng ta. Nó làm tăng nguy cơ quá khớp và có thể dẫn đến khả năng dự đoán kém hơn của mô hình.\n",
    "* Trong cách tiếp cận cơ bản, chúng ta chỉ tách một phần của tập dữ liệu làm tập kiểm tra. Điều này có nghĩa là chúng ta lại dựa vào một lần thực hiện của một quá trình ngẫu nhiên nào đó. Điều này có nghĩa là chúng ta có thể nhận được kết quả rất khác nhau mỗi lần ngay cả khi chúng ta lặp lại toàn bộ quy trình huấn luyện giống hệt nhau.\n",
    "\n",
    "Các vấn đề nói trên có hai giải pháp khả thi. Chúng ta có thể thực hiện k-fold huấn luyện-kiểm tra/kiểm định. Nó làm giảm phương sai của kết quả trong tập kiểm tra. Mặt khác, nó làm tăng độ phức tạp của quy trình ML của chúng ta và không làm giảm vấn đề giảm số lượng quan sát để huấn luyện.\n",
    "\n",
    "Ngoài ra, chúng ta có thể nói rằng tập kiểm tra thực sự của chúng ta sẽ đến sau. Chúng ta sẽ chỉ cần đợi dữ liệu mới từ thị trường sẽ được tạo ra vào thời điểm mô hình của chúng ta được tạo. Loại dữ liệu kiểm tra này là hoàn hảo vì nó đến từ tương lai. Đây thường là một lựa chọn tốt vì k-fold CV lặp lại được triển khai chính xác sẽ đủ để chọn mô hình tốt nhất và tối ưu hóa các siêu tham số của nó.\n",
    "\n",
    "Cuối cùng, chúng ta cần chọn một giải pháp phù hợp cho một vấn đề cụ thể, ghi nhớ loại thuật toán chúng ta đang lên kế hoạch sử dụng và hiểu nhu cầu kinh doanh đối với vấn đề ML hiện tại."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thiết lập Kiểm định Chéo như thế nào?\n",
    "Mặc dù về mặt lý thuyết, quy trình CV rất đơn giản, nhưng rất dễ mắc phải những sai lầm mà trong một số trường hợp sẽ rất tốn kém.\n",
    "\n",
    "### Phân tầng (Stratify)\n",
    "Một trong những vấn đề chúng ta có thể gặp phải là sự **cân bằng của các nhãn** trong tập huấn luyện và kiểm định. Điều này đặc biệt quan trọng khi chúng ta làm việc với **dữ liệu mất cân bằng (imbalanced data)**.\n",
    "\n",
    "* Trong các tập dữ liệu có ít nhãn \"1\" nói chung, rất dễ hình dung một tình huống mà chúng ta rút ra không có hoặc tương đối rất ít nhãn \"1\".\n",
    "* Nó có thể dẫn đến **thiên vị** và/hoặc **phương sai lớn** của chỉ số đánh giá trên tập dữ liệu kiểm định.\n",
    "* Vì vấn đề này, chúng ta thường buộc phải cân bằng bằng cách sử dụng **phân tầng (stratification)**.\n",
    "\n",
    "**Ví dụ:** Trong trường hợp 5-fold CV, bạn có thể hình dung rằng chúng ta thực hiện việc chia tách tập huấn luyện và kiểm định riêng cho nhãn \"1\" và \"0\". Điều này đảm bảo **sự cân bằng đồng đều** trong cả hai nhóm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phân tích Chuỗi thời gian (Time series analysis)\n",
    "Bộ dữ liệu khó thiết lập kiểm định chéo phù hợp nhất là **phân tích chuỗi thời gian**.\n",
    "* Do thực tế là tất cả các quan sát thường được **liên kết** (không độc lập theo thời gian), mỗi quan sát phụ thuộc vào các giá trị trước đó.\n",
    "* Vì đặc tính này, chúng ta không thể chỉ đơn giản là rút ngẫu nhiên các quan sát vào hai mẫu và sử dụng các hoán vị khác nhau.\n",
    "* Các quan sát liền kề nhau về mặt thời gian chứa quá nhiều thông tin về nhau để có thể đặt chúng vào các tập khác nhau (huấn luyện và kiểm định).\n",
    "\n",
    "Một giải pháp là thực hiện việc **chia tách tự nhiên dựa trên thời gian**.\n",
    "* Chúng ta sử dụng các quan sát **\"quá khứ\"** làm dữ liệu huấn luyện (training data) và các quan sát **\"tương lai\"** làm tập kiểm định (validation set).\n",
    "* Giống như trong hình minh họa dưới đây.\n",
    "<img src=\"img/cvTimeSeries.png\" width=\"50%\">\n",
    "\n",
    "Nguồn: https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rò rỉ Thông tin (Information leakage)\n",
    "Sự **độc lập** của các quan sát giữa tập huấn luyện (training) và tập kiểm định (validation) là một yêu cầu quan trọng để quy trình mang lại kết quả mong đợi. Tầm quan trọng của nó được thể hiện rõ ràng trong phân tích chuỗi thời gian. Tuy nhiên, **rò rỉ thông tin** vẫn có thể xảy ra ngay cả trong các bài toán phân loại và hồi quy thông thường, khi yêu cầu độc lập không được đáp ứng.\n",
    "\n",
    "Trong trường hợp này, khi các quan sát không độc lập, chúng ta không thể coi tập kiểm định là dữ liệu *ngoài mẫu* (out of sample) thực sự.\n",
    "* Điều này dẫn đến việc **đánh giá thấp lỗi** trên tập kiểm định.\n",
    "* Nó có thể dẫn đến việc **đánh giá quá cao** mô hình và kết quả là một mô hình đơn giản là không hoạt động khi triển khai.\n",
    "\n",
    "**Ví dụ về cấu trúc dữ liệu dễ gây rò rỉ:**\n",
    "* Có nhiều quan sát cho cùng một người trong bộ dữ liệu.\n",
    "* Sử dụng dữ liệu từ nhiều người trong cùng một hộ gia đình.\n",
    "\n",
    "Các lỗi trong việc thiết lập quy trình kiểm định chéo rất **dễ mắc phải và khó phát hiện**. Ngay cả trong các cuộc thi trên Kaggle, rò rỉ dữ liệu vẫn xảy ra theo thời gian mặc dù các tập dữ liệu được chuẩn bị bởi các chuyên gia có kinh nghiệm. **Luôn cố gắng tìm hiểu dữ liệu của bạn và thiết lập CV cẩn thận**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huấn luyện - Kiểm định - Kiểm tra (Train Validate Test)\n",
    "Tại thời điểm này, chúng ta nên chỉ ra một vấn đề quan trọng với các tuyên bố được đưa ra ở phần đầu. Chúng ta đã nói rằng chúng ta có thể sử dụng kết quả từ tập kiểm định để chọn giá trị cho các siêu tham số của thuật toán. Tuy nhiên, điều này có nghĩa là chúng ta đang chuyển thông tin từ tập kiểm định sang huấn luyện (**rò rỉ dữ liệu - data leakage**). Việc này, nếu bị lạm dụng, có thể dẫn đến đánh giá thiên vị và mô hình không tối ưu.\n",
    "\n",
    "Thông thường, rò rỉ thông tin là nhỏ và không đáng kể, tuy nhiên trong những trường hợp chúng ta phụ thuộc nhiều vào tập kiểm định để điều chỉnh thuật toán, chúng ta nên sử dụng quy trình **huấn luyện, kiểm định, kiểm tra** (train, validate, test). Ý tưởng hoàn toàn giống như kiểm định chéo nhưng chúng ta thêm một bước nữa. Chúng ta giữ lại một phần dữ liệu (test) ở phần đầu mà chúng ta không sử dụng chút nào trong quá trình huấn luyện. Sau đó, cuối cùng, khi mọi thứ đã sẵn sàng, chúng ta kiểm tra mô hình của mình bằng tập kiểm tra này.\n",
    "<img src=\"img/cvTVT.png\">\n",
    "Source:https://am207.github.io/2017/wiki/validation.html\n",
    "\n",
    "### Huấn luyện - Kiểm định - Kiểm tra trong thực tế\n",
    "Việc chia tập dữ liệu của chúng ta thành ba phần, giống như mọi thứ trong học máy, đều có cả ưu điểm và nhược điểm. Rõ ràng, lợi thế chính là có được **đánh giá thuần túy và đáng tin cậy** về mô hình. Chúng ta cũng không thể quên những nhược điểm:\n",
    "* Việc tách tập kiểm tra làm giảm số lượng quan sát trong tập huấn luyện của chúng ta. Nó làm tăng nguy cơ quá khớp và có thể dẫn đến khả năng dự đoán kém hơn của mô hình.\n",
    "* Trong cách tiếp cận cơ bản, chúng ta chỉ tách một phần của tập dữ liệu làm tập kiểm tra. Điều này có nghĩa là chúng ta lại dựa vào một lần thực hiện của một quá trình ngẫu nhiên nào đó. Điều này có nghĩa là chúng ta có thể nhận được kết quả rất khác nhau mỗi lần ngay cả khi chúng ta lặp lại toàn bộ quy trình huấn luyện giống hệt nhau.\n",
    "\n",
    "Các vấn đề nói trên có hai giải pháp khả thi.\n",
    "1.  Chúng ta có thể thực hiện **k-fold huấn luyện-kiểm tra/kiểm định** (train-test/validate). Nó làm giảm phương sai của kết quả trong tập kiểm tra. Mặt khác, nó làm tăng độ phức tạp của quy trình ML của chúng ta và không làm giảm vấn đề giảm số lượng quan sát để huấn luyện.\n",
    "2.  Ngoài ra, chúng ta có thể nói rằng tập kiểm tra thực sự của chúng ta sẽ đến sau. Chúng ta sẽ chỉ cần đợi **dữ liệu mới từ thị trường** sẽ được tạo ra vào thời điểm mô hình của chúng ta được tạo. Loại dữ liệu kiểm tra này là hoàn hảo vì nó đến từ tương lai. Đây thường là một lựa chọn tốt vì k-fold CV lặp lại được triển khai chính xác sẽ đủ để chọn mô hình tốt nhất và tối ưu hóa các siêu tham số của nó.\n",
    "\n",
    "Cuối cùng, chúng ta cần chọn một giải pháp phù hợp cho một vấn đề cụ thể, ghi nhớ loại thuật toán chúng ta đang lên kế hoạch sử dụng và hiểu nhu cầu kinh doanh đối với vấn đề ML hiện tại."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation in Python\n",
    "Most of the methods for cross validation are really well implemented in sklearn.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/cross_validation.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:21:43.121407Z",
     "start_time": "2021-03-24T16:21:42.550259Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "plt.style.use('seaborn-ticks')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get back to our medical set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:21:45.169405Z",
     "start_time": "2021-03-24T16:21:45.077783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\programy\\Python39\\lib\\site-packages\\scipy\\.libs\n",
      "(35072, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UMARSTAT</th>\n",
       "      <th>UCUREMP</th>\n",
       "      <th>UCURNINS</th>\n",
       "      <th>USATMED</th>\n",
       "      <th>URELATE</th>\n",
       "      <th>REGION</th>\n",
       "      <th>STATE</th>\n",
       "      <th>HHID</th>\n",
       "      <th>FHOSP</th>\n",
       "      <th>FDENT</th>\n",
       "      <th>FEMER</th>\n",
       "      <th>FDOCT</th>\n",
       "      <th>UIMMSTAT</th>\n",
       "      <th>U_USBORN</th>\n",
       "      <th>UAGE</th>\n",
       "      <th>U_FTPT</th>\n",
       "      <th>U_WKSLY</th>\n",
       "      <th>U_HRSLY</th>\n",
       "      <th>U_USHRS</th>\n",
       "      <th>HEARNVAL</th>\n",
       "      <th>HOTHVAL</th>\n",
       "      <th>HRETVAL</th>\n",
       "      <th>HSSVAL</th>\n",
       "      <th>HWSVAL</th>\n",
       "      <th>UBRACE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>UEDUC3</th>\n",
       "      <th>CEYES</th>\n",
       "      <th>CHAIR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Never married</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>2</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>WI</td>\n",
       "      <td>55616128</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>US-born citizen</td>\n",
       "      <td>Yes</td>\n",
       "      <td>22</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>52.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>No HS diploma or GED</td>\n",
       "      <td>hazel</td>\n",
       "      <td>brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Separated</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>2</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>WI</td>\n",
       "      <td>54704000</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>US-born citizen</td>\n",
       "      <td>Yes</td>\n",
       "      <td>30</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>52.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>31468</td>\n",
       "      <td>5950</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31468</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>HS diploma or GED, no bachelor's degree</td>\n",
       "      <td>blue</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Married_live together</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>5</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>WI</td>\n",
       "      <td>57874272</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US-born citizen</td>\n",
       "      <td>Yes</td>\n",
       "      <td>33</td>\n",
       "      <td>Part-time</td>\n",
       "      <td>52.0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>24700</td>\n",
       "      <td>11340</td>\n",
       "      <td>0</td>\n",
       "      <td>4920</td>\n",
       "      <td>24700</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>No HS diploma or GED</td>\n",
       "      <td>brown</td>\n",
       "      <td>brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Divorced</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Little dissatisfied</td>\n",
       "      <td>4</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>WI</td>\n",
       "      <td>54106816</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>US-born citizen</td>\n",
       "      <td>Yes</td>\n",
       "      <td>41</td>\n",
       "      <td>Part-time</td>\n",
       "      <td>43.0</td>\n",
       "      <td>40</td>\n",
       "      <td>25</td>\n",
       "      <td>60000</td>\n",
       "      <td>39002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60000</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>No HS diploma or GED</td>\n",
       "      <td>brown</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Never married</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>0</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>WI</td>\n",
       "      <td>54569152</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>US-born citizen</td>\n",
       "      <td>Yes</td>\n",
       "      <td>34</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>52.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>55280</td>\n",
       "      <td>4200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55280</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>HS diploma or GED, no bachelor's degree</td>\n",
       "      <td>brown</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                UMARSTAT UCUREMP UCURNINS              USATMED  URELATE  \\\n",
       "0          Never married      No      Yes       Very satisfied        2   \n",
       "1              Separated     Yes       No       Very satisfied        2   \n",
       "2  Married_live together      No       No       Very satisfied        5   \n",
       "3               Divorced      No      Yes  Little dissatisfied        4   \n",
       "4          Never married     Yes       No       Very satisfied        0   \n",
       "\n",
       "    REGION STATE      HHID FHOSP  FDENT  FEMER  FDOCT         UIMMSTAT  \\\n",
       "0  Midwest    WI  55616128    No      0      0      0  US-born citizen   \n",
       "1  Midwest    WI  54704000    No      2      0      0  US-born citizen   \n",
       "2  Midwest    WI  57874272    No      0      1      0  US-born citizen   \n",
       "3  Midwest    WI  54106816    No      0      0      1  US-born citizen   \n",
       "4  Midwest    WI  54569152    No      2      0      0  US-born citizen   \n",
       "\n",
       "  U_USBORN  UAGE     U_FTPT  U_WKSLY  U_HRSLY  U_USHRS  HEARNVAL  HOTHVAL  \\\n",
       "0      Yes    22  Full-time     52.0       40       40         0        0   \n",
       "1      Yes    30  Full-time     52.0       40       40     31468     5950   \n",
       "2      Yes    33  Part-time     52.0       30       30     24700    11340   \n",
       "3      Yes    41  Part-time     43.0       40       25     60000    39002   \n",
       "4      Yes    34  Full-time     52.0       40       40     55280     4200   \n",
       "\n",
       "   HRETVAL  HSSVAL  HWSVAL UBRACE  GENDER  \\\n",
       "0        0       0       0  White  Female   \n",
       "1        0       0   31468  White  Female   \n",
       "2        0    4920   24700  White    Male   \n",
       "3        0       0   60000  Black  Female   \n",
       "4        0       0   55280  Black    Male   \n",
       "\n",
       "                                    UEDUC3  CEYES  CHAIR  \n",
       "0                     No HS diploma or GED  hazel  brown  \n",
       "1  HS diploma or GED, no bachelor's degree   blue  black  \n",
       "2                     No HS diploma or GED  brown  brown  \n",
       "3                     No HS diploma or GED  brown  black  \n",
       "4  HS diploma or GED, no bachelor's degree  brown  black  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd C:\n",
    "medical = pd.read_csv(\"medical_care.csv\")\n",
    "print(medical.shape)\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "medical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets encode UCURNINS variable and run the regression on our full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:21:46.204766Z",
     "start_time": "2021-03-24T16:21:46.197124Z"
    }
   },
   "outputs": [],
   "source": [
    "medical[\"UCURNINS\"] = (medical.UCURNINS == \"Yes\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:21:54.887758Z",
     "start_time": "2021-03-24T16:21:54.064105Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>UCURNINS</td>     <th>  No. Observations:  </th>  <td> 35072</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td> 35036</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>    35</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>         <td>Logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -11126.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Tue, 29 Mar 2022</td> <th>  Deviance:          </th> <td>  22251.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>19:37:05</td>     <th>  Pearson chi2:      </th> <td>4.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>6</td>        <th>  Pseudo R-squ. (CS):</th>  <td>0.1742</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                          <td></td>                             <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                         <td>    0.9019</td> <td>    0.216</td> <td>    4.166</td> <td> 0.000</td> <td>    0.478</td> <td>    1.326</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Married, do not live together]</th>         <td>   -0.4264</td> <td>    0.166</td> <td>   -2.563</td> <td> 0.010</td> <td>   -0.752</td> <td>   -0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Married_live together]</th>                 <td>   -0.8331</td> <td>    0.056</td> <td>  -14.774</td> <td> 0.000</td> <td>   -0.944</td> <td>   -0.723</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Never married]</th>                         <td>   -0.3337</td> <td>    0.064</td> <td>   -5.210</td> <td> 0.000</td> <td>   -0.459</td> <td>   -0.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Partnership]</th>                           <td>    0.3966</td> <td>    0.085</td> <td>    4.677</td> <td> 0.000</td> <td>    0.230</td> <td>    0.563</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Separated]</th>                             <td>   -0.0527</td> <td>    0.095</td> <td>   -0.553</td> <td> 0.580</td> <td>   -0.239</td> <td>    0.134</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Unknown]</th>                               <td>    0.6720</td> <td>    0.388</td> <td>    1.732</td> <td> 0.083</td> <td>   -0.089</td> <td>    1.433</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Widowed]</th>                               <td>   -0.1727</td> <td>    0.144</td> <td>   -1.197</td> <td> 0.231</td> <td>   -0.455</td> <td>    0.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>USATMED[T.Little satisfied]</th>                       <td>   -0.4597</td> <td>    0.063</td> <td>   -7.324</td> <td> 0.000</td> <td>   -0.583</td> <td>   -0.337</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>USATMED[T.No opinion]</th>                             <td>    0.6913</td> <td>    0.098</td> <td>    7.025</td> <td> 0.000</td> <td>    0.498</td> <td>    0.884</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>USATMED[T.Very dissatisfied]</th>                      <td>    0.5547</td> <td>    0.092</td> <td>    6.017</td> <td> 0.000</td> <td>    0.374</td> <td>    0.735</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>USATMED[T.Very satisfied]</th>                         <td>   -0.7287</td> <td>    0.062</td> <td>  -11.668</td> <td> 0.000</td> <td>   -0.851</td> <td>   -0.606</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REGION[T.Northeast]</th>                               <td>   -0.0486</td> <td>    0.057</td> <td>   -0.858</td> <td> 0.391</td> <td>   -0.159</td> <td>    0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REGION[T.South]</th>                                   <td>    0.7221</td> <td>    0.047</td> <td>   15.439</td> <td> 0.000</td> <td>    0.630</td> <td>    0.814</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REGION[T.West]</th>                                    <td>    0.4020</td> <td>    0.049</td> <td>    8.143</td> <td> 0.000</td> <td>    0.305</td> <td>    0.499</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FHOSP[T.Yes]</th>                                      <td>   -0.2438</td> <td>    0.075</td> <td>   -3.254</td> <td> 0.001</td> <td>   -0.391</td> <td>   -0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UIMMSTAT[T.Foreign-born, non-citizen]</th>             <td>    0.7271</td> <td>    0.089</td> <td>    8.172</td> <td> 0.000</td> <td>    0.553</td> <td>    0.901</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UIMMSTAT[T.US-born citizen]</th>                       <td>   -0.5706</td> <td>    0.080</td> <td>   -7.114</td> <td> 0.000</td> <td>   -0.728</td> <td>   -0.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_FTPT[T.Part-time]</th>                               <td>    0.4966</td> <td>    0.058</td> <td>    8.526</td> <td> 0.000</td> <td>    0.382</td> <td>    0.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UBRACE[T.Asian/Pacific Islander]</th>                  <td>   -0.9828</td> <td>    0.161</td> <td>   -6.104</td> <td> 0.000</td> <td>   -1.298</td> <td>   -0.667</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UBRACE[T.Black]</th>                                   <td>   -0.4685</td> <td>    0.126</td> <td>   -3.710</td> <td> 0.000</td> <td>   -0.716</td> <td>   -0.221</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UBRACE[T.White]</th>                                   <td>   -0.5119</td> <td>    0.118</td> <td>   -4.338</td> <td> 0.000</td> <td>   -0.743</td> <td>   -0.281</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UEDUC3[T.HS diploma or GED, no bachelor's degree]</th> <td>    0.8375</td> <td>    0.053</td> <td>   15.828</td> <td> 0.000</td> <td>    0.734</td> <td>    0.941</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UEDUC3[T.No HS diploma or GED]</th>                    <td>    1.5713</td> <td>    0.063</td> <td>   24.881</td> <td> 0.000</td> <td>    1.448</td> <td>    1.695</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GENDER[T.Male]</th>                                    <td>    0.1413</td> <td>    0.039</td> <td>    3.652</td> <td> 0.000</td> <td>    0.065</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>URELATE</th>                                           <td>    0.0161</td> <td>    0.012</td> <td>    1.350</td> <td> 0.177</td> <td>   -0.007</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FDENT</th>                                             <td>   -0.3159</td> <td>    0.015</td> <td>  -21.188</td> <td> 0.000</td> <td>   -0.345</td> <td>   -0.287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FEMER</th>                                             <td>    0.0871</td> <td>    0.022</td> <td>    3.976</td> <td> 0.000</td> <td>    0.044</td> <td>    0.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FDOCT</th>                                             <td>   -0.1353</td> <td>    0.008</td> <td>  -16.404</td> <td> 0.000</td> <td>   -0.151</td> <td>   -0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UAGE</th>                                              <td>   -0.0182</td> <td>    0.002</td> <td>   -9.766</td> <td> 0.000</td> <td>   -0.022</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_WKSLY</th>                                           <td>   -0.0197</td> <td>    0.002</td> <td>  -12.732</td> <td> 0.000</td> <td>   -0.023</td> <td>   -0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_USHRS</th>                                           <td>    0.0022</td> <td>    0.002</td> <td>    1.234</td> <td> 0.217</td> <td>   -0.001</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HOTHVAL</th>                                           <td>-1.179e-07</td> <td> 1.57e-06</td> <td>   -0.075</td> <td> 0.940</td> <td>-3.19e-06</td> <td> 2.95e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HRETVAL</th>                                           <td> 5.256e-06</td> <td> 3.18e-06</td> <td>    1.650</td> <td> 0.099</td> <td>-9.86e-07</td> <td> 1.15e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HSSVAL</th>                                            <td>-5.631e-06</td> <td> 3.77e-06</td> <td>   -1.494</td> <td> 0.135</td> <td> -1.3e-05</td> <td> 1.76e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HWSVAL</th>                                            <td>  9.47e-09</td> <td> 3.24e-07</td> <td>    0.029</td> <td> 0.977</td> <td>-6.26e-07</td> <td> 6.45e-07</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:               UCURNINS   No. Observations:                35072\n",
       "Model:                            GLM   Df Residuals:                    35036\n",
       "Model Family:                Binomial   Df Model:                           35\n",
       "Link Function:                  Logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -11126.\n",
       "Date:                Tue, 29 Mar 2022   Deviance:                       22251.\n",
       "Time:                        19:37:05   Pearson chi2:                 4.05e+04\n",
       "No. Iterations:                     6   Pseudo R-squ. (CS):             0.1742\n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================================================\n",
       "                                                        coef    std err          z      P>|z|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                             0.9019      0.216      4.166      0.000       0.478       1.326\n",
       "UMARSTAT[T.Married, do not live together]            -0.4264      0.166     -2.563      0.010      -0.752      -0.100\n",
       "UMARSTAT[T.Married_live together]                    -0.8331      0.056    -14.774      0.000      -0.944      -0.723\n",
       "UMARSTAT[T.Never married]                            -0.3337      0.064     -5.210      0.000      -0.459      -0.208\n",
       "UMARSTAT[T.Partnership]                               0.3966      0.085      4.677      0.000       0.230       0.563\n",
       "UMARSTAT[T.Separated]                                -0.0527      0.095     -0.553      0.580      -0.239       0.134\n",
       "UMARSTAT[T.Unknown]                                   0.6720      0.388      1.732      0.083      -0.089       1.433\n",
       "UMARSTAT[T.Widowed]                                  -0.1727      0.144     -1.197      0.231      -0.455       0.110\n",
       "USATMED[T.Little satisfied]                          -0.4597      0.063     -7.324      0.000      -0.583      -0.337\n",
       "USATMED[T.No opinion]                                 0.6913      0.098      7.025      0.000       0.498       0.884\n",
       "USATMED[T.Very dissatisfied]                          0.5547      0.092      6.017      0.000       0.374       0.735\n",
       "USATMED[T.Very satisfied]                            -0.7287      0.062    -11.668      0.000      -0.851      -0.606\n",
       "REGION[T.Northeast]                                  -0.0486      0.057     -0.858      0.391      -0.159       0.062\n",
       "REGION[T.South]                                       0.7221      0.047     15.439      0.000       0.630       0.814\n",
       "REGION[T.West]                                        0.4020      0.049      8.143      0.000       0.305       0.499\n",
       "FHOSP[T.Yes]                                         -0.2438      0.075     -3.254      0.001      -0.391      -0.097\n",
       "UIMMSTAT[T.Foreign-born, non-citizen]                 0.7271      0.089      8.172      0.000       0.553       0.901\n",
       "UIMMSTAT[T.US-born citizen]                          -0.5706      0.080     -7.114      0.000      -0.728      -0.413\n",
       "U_FTPT[T.Part-time]                                   0.4966      0.058      8.526      0.000       0.382       0.611\n",
       "UBRACE[T.Asian/Pacific Islander]                     -0.9828      0.161     -6.104      0.000      -1.298      -0.667\n",
       "UBRACE[T.Black]                                      -0.4685      0.126     -3.710      0.000      -0.716      -0.221\n",
       "UBRACE[T.White]                                      -0.5119      0.118     -4.338      0.000      -0.743      -0.281\n",
       "UEDUC3[T.HS diploma or GED, no bachelor's degree]     0.8375      0.053     15.828      0.000       0.734       0.941\n",
       "UEDUC3[T.No HS diploma or GED]                        1.5713      0.063     24.881      0.000       1.448       1.695\n",
       "GENDER[T.Male]                                        0.1413      0.039      3.652      0.000       0.065       0.217\n",
       "URELATE                                               0.0161      0.012      1.350      0.177      -0.007       0.039\n",
       "FDENT                                                -0.3159      0.015    -21.188      0.000      -0.345      -0.287\n",
       "FEMER                                                 0.0871      0.022      3.976      0.000       0.044       0.130\n",
       "FDOCT                                                -0.1353      0.008    -16.404      0.000      -0.151      -0.119\n",
       "UAGE                                                 -0.0182      0.002     -9.766      0.000      -0.022      -0.015\n",
       "U_WKSLY                                              -0.0197      0.002    -12.732      0.000      -0.023      -0.017\n",
       "U_USHRS                                               0.0022      0.002      1.234      0.217      -0.001       0.006\n",
       "HOTHVAL                                           -1.179e-07   1.57e-06     -0.075      0.940   -3.19e-06    2.95e-06\n",
       "HRETVAL                                            5.256e-06   3.18e-06      1.650      0.099   -9.86e-07    1.15e-05\n",
       "HSSVAL                                            -5.631e-06   3.77e-06     -1.494      0.135    -1.3e-05    1.76e-06\n",
       "HWSVAL                                              9.47e-09   3.24e-07      0.029      0.977   -6.26e-07    6.45e-07\n",
       "=====================================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = sm.GLM.from_formula(formula = 'UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + ' + \n",
    "                          'UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + ' + \n",
    "                          'UEDUC3 + GENDER',\n",
    "                          data = medical,\n",
    "                          family = sm.families.Binomial())\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5680/952132274.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đối với các lần chia đơn giản, chúng ta có thể sử dụng **train_test_split**. Phương thức này sẽ chia tập dữ liệu thành các biến đầu vào (features) và cột biến phụ thuộc (dependant variable) của chúng ta.\n",
    "* Chúng ta có thể định nghĩa **random_state** hoặc khởi tạo nó bằng một số ngẫu nhiên.\n",
    "* Việc kiểm soát tính ngẫu nhiên của các lần chia có thể giúp ích rất nhiều cho khả năng **tái lập kết quả** trong một số trường hợp.\n",
    "* Để thấy sự khác biệt, hãy chạy lại một vài lần ô phía trên và phía dưới."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:22:55.211066Z",
     "start_time": "2021-03-24T16:22:54.508044Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(medical,\n",
    "                                                    medical.UCURNINS,\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = random.randint(0, 1000))\n",
    "print(X_train.shape, X_test.shape)\n",
    "mod = sm.GLM.from_formula(formula = 'UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + ' + \n",
    "                          'UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + ' + \n",
    "                          'UEDUC3 + GENDER',\n",
    "                          data = X_train,\n",
    "                          family = sm.families.Binomial())\n",
    "res = mod.fit()\n",
    "res.summary()\n",
    "preds = res.predict(X_test)\n",
    "\n",
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Chúng ta có thể rất dễ dàng áp dụng **phân tầng (stratify)** chỉ với một đối số. Chúng ta chỉ cần đặt cột cho việc phân tầng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:26:27.953257Z",
     "start_time": "2021-03-24T16:25:50.835743Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for k in range(100) :\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(medical,\n",
    "                                                        medical.UCURNINS,\n",
    "                                                        stratify = medical.UCURNINS,\n",
    "                                                        test_size = 0.3,\n",
    "                                                        random_state = random.randint(0, 10000))\n",
    "    # print(X_train.shape, X_test.shape)\n",
    "    mod = sm.GLM.from_formula(formula = 'UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + ' + \n",
    "                              'UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + ' + \n",
    "                              'UEDUC3 + GENDER',\n",
    "                              data = X_train,\n",
    "                              family = sm.families.Binomial())\n",
    "    res = mod.fit()\n",
    "    res.summary()\n",
    "    preds = res.predict(X_test)\n",
    "    scores.append(roc_auc_score(y_test, preds))\n",
    "    # print(scores[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như chúng ta có thể thấy, trong trường hợp hồi quy logistic trên bộ dữ liệu này, có **rất ít sự khác biệt** khi có và không có phân tầng. Nhãn \"1\" của chúng ta được đại diện đủ tốt (**5 000 quan sát**) để tính ngẫu nhiên hoạt động đủ tốt với tỷ lệ chia 0.7 và 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medical.UCURNINS.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see some diagnostics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = scores, columns = ['scores'])\n",
    "print(df.scores.describe())\n",
    "print(f'Kurtosis \\t %s' %round(df.scores.kurtosis(), 5)) # This one returns 'normalized' kurtosis (i.e. 0 instead of 3).\n",
    "print(f'Skewness \\t %s' %round(df.scores.skew(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "ax = sns.displot(data = df.scores, kde = True, label = 'empirical hist')\n",
    "x0, x1 = ax.ax.get_xlim()\n",
    "x_pdf = np.linspace(x0, x1, len(df))\n",
    "y_pdf = stats.norm.pdf(x_pdf, df.scores.mean(), df.scores.std())\n",
    "ax.ax.plot(x_pdf, y_pdf, 'r', lw = 2, label = 'normal pdf')\n",
    "ax.ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats \n",
    "\n",
    "alpha = 1e-3\n",
    "\n",
    "k2, p = stats.normaltest(df.scores) # Kurtosis and skewness normality test.\n",
    "\n",
    "if p < alpha :\n",
    "\n",
    "    print('Kurtosis, Skewness test: The null hypothesis about normality can be rejected.')\n",
    "\n",
    "else:\n",
    "\n",
    "    print('Kurtosis, Skewness test: The null hypothesis about normality can not be rejected.')\n",
    "\n",
    "# Kolmogorov Smirnov normality test.\n",
    "ks = stats.kstest(df.scores, 'norm')\n",
    "\n",
    "if ks[1] < alpha :\n",
    "\n",
    "    print('Kolmogorov Smirnov test: The null hypothesis about normality can be rejected.')\n",
    "\n",
    "else:\n",
    "\n",
    "    print('Kolmogorov Smirnov test: The null hypothesis about normality can not be rejected.')\n",
    "\n",
    "# Do you remember that @kstest function requires NORMALIZATION of sample?\n",
    "ks = stats.kstest((df.scores - df.scores.mean()) / df.scores.std(), 'norm')\n",
    "\n",
    "if ks[1] < alpha :\n",
    "\n",
    "    print('Kolmogorov Smirnov (normalized sample) test: The null hypothesis about normality can be rejected.')\n",
    "\n",
    "else:\n",
    "\n",
    "    print('Kolmogorov Smirnov (normalized sample) test: The null hypothesis about normality can not be rejected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ chúng ta có thể kiểm tra **xu hướng quá khớp (overfitting)** trong hồi quy logistic.\n",
    "* Như bạn có thể thấy bên dưới, chúng ta cần giảm kích thước tập dữ liệu huấn luyện xuống chỉ còn **10%** để thực sự bắt đầu quá khớp.\n",
    "* Điều này cho thấy khả năng **chống lại vấn đề quá khớp** là rất tốt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:27:56.503508Z",
     "start_time": "2021-03-24T16:27:50.716171Z"
    }
   },
   "outputs": [],
   "source": [
    "for k in range(1, 10) :\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(medical,\n",
    "                                                        medical.UCURNINS,\n",
    "                                                        test_size = 0.1 * k,\n",
    "                                                        random_state = 0)\n",
    "    mod = sm.GLM.from_formula(formula = 'UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + ' + \n",
    "                              'UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + ' + \n",
    "                              'UEDUC3 + GENDER',\n",
    "                              data = X_train,\n",
    "                              family = sm.families.Binomial())\n",
    "    res = mod.fit()\n",
    "    predsTrain = res.predict(X_train)\n",
    "    preds = res.predict(X_test)\n",
    "    print(\"Train AUC:\", round(roc_auc_score(y_train, predsTrain), 4), \"Valid AUC:\", round(roc_auc_score(y_test, preds), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đối với k-fold CV, chúng ta không nhận được tập dữ liệu mà là các **chỉ mục (indices)** cho các tập dữ liệu. Điều này được thực hiện để tránh **trùng lặp tập dữ liệu không cần thiết**.\n",
    "\n",
    "Hãy chạy 10-fold CV và xem kết quả. Đối với k-fold, chúng ta cần thêm **shuffle = True**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:29:09.719179Z",
     "start_time": "2021-03-24T16:29:01.573294Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 10, shuffle = True, random_state = random.randint(0, 10000))\n",
    "\n",
    "for train, test in kf.split(medical.index.values) :\n",
    "    \n",
    "    mod = sm.GLM.from_formula(formula = 'UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + ' + \n",
    "                              'UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + ' + \n",
    "                              'UEDUC3 + GENDER',\n",
    "                              data = medical.iloc[train],\n",
    "                              family = sm.families.Binomial())\n",
    "    res = mod.fit()\n",
    "    predsTrain = res.predict(medical.iloc[train])\n",
    "    preds = res.predict(medical.iloc[test])\n",
    "    print(\"Train AUC:\", round(roc_auc_score(medical.iloc[train].UCURNINS, predsTrain), 4), \"Valid AUC:\",\n",
    "          round(roc_auc_score(medical.iloc[test].UCURNINS, preds), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Như chúng ta có thể thấy, kết quả **thay đổi rất nhiều từ mẫu này sang mẫu khác**. Chúng ta thấy các giá trị từ 0.81 đến 0.838 trên các tập kiểm định.\n",
    "\n",
    "Để hiểu rõ hơn về vấn đề **phương sai**, hãy xem kết quả cho 5-fold CV.\n",
    "* Phương sai của kết quả dường như **thấp hơn một chút** đối với 10-fold CV.\n",
    "* Điều này là tự nhiên vì hồi quy logistic không quá khớp nhiều và chúng ta **tăng kích thước của tập kiểm định** (điều này ổn định kết quả trên các mẫu kiểm định)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:31:04.729383Z",
     "start_time": "2021-03-24T16:31:00.883441Z"
    }
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 5, shuffle = True, random_state = random.randint(0, 10000))\n",
    "\n",
    "for train, test in kf.split(medical.index.values) :\n",
    "    \n",
    "    mod = sm.GLM.from_formula(formula = 'UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + ' + \n",
    "                              'UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + ' + \n",
    "                              'UEDUC3 + GENDER',\n",
    "                              data = medical.iloc[train],\n",
    "                              family = sm.families.Binomial())    \n",
    "    res = mod.fit()\n",
    "    predsTrain = res.predict(medical.iloc[train])\n",
    "    preds = res.predict(medical.iloc[test])\n",
    "    print(\"Train AUC:\", round(roc_auc_score(medical.iloc[train].UCURNINS, predsTrain), 4),\n",
    "          \"Valid AUC:\", round(roc_auc_score(medical.iloc[test].UCURNINS, preds), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hãy xem vấn đề **phương sai** chi tiết hơn. Hãy chạy **10-fold CV 10 lần** và xem phương sai của các giá trị trung bình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:33:31.620760Z",
     "start_time": "2021-03-24T16:32:10.356653Z"
    }
   },
   "outputs": [],
   "source": [
    "for z in range(10) :\n",
    "    \n",
    "    trainRes = []\n",
    "    valRes = []\n",
    "    kf = KFold(n_splits = 10, shuffle = True, random_state = random.randint(0, 10000))\n",
    "    \n",
    "    for train, test in kf.split(medical.index.values):\n",
    "        \n",
    "        mod = sm.GLM.from_formula(formula = 'UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + ' + \n",
    "                              'UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + ' + \n",
    "                              'UEDUC3 + GENDER',\n",
    "                              data = medical.iloc[train],\n",
    "                              family = sm.families.Binomial())          \n",
    "        res = mod.fit()\n",
    "        predsTrain = res.predict(medical.iloc[train])\n",
    "        preds = res.predict(medical.iloc[test])\n",
    "        trainRes.append(roc_auc_score(medical.iloc[train].UCURNINS, predsTrain))\n",
    "        valRes.append(roc_auc_score(medical.iloc[test].UCURNINS, preds))\n",
    "        \n",
    "    print(\"Train AUC:\", round(np.mean(trainRes), 4), \"Valid AUC:\", round(np.mean(valRes), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same for 5-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:34:14.913581Z",
     "start_time": "2021-03-24T16:33:35.900569Z"
    }
   },
   "outputs": [],
   "source": [
    "for z in range(10) :\n",
    "    \n",
    "    trainRes = []\n",
    "    valRes = []\n",
    "    kf = KFold(n_splits = 5, shuffle = True, random_state = random.randint(0, 10000))\n",
    "    \n",
    "    for train, test in kf.split(medical.index.values) :\n",
    "        \n",
    "        mod = sm.GLM.from_formula(formula = 'UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + ' + \n",
    "                              'UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + ' + \n",
    "                              'UEDUC3 + GENDER',\n",
    "                              data = medical.iloc[train],\n",
    "                              family = sm.families.Binomial()) \n",
    "        res = mod.fit()\n",
    "        predsTrain = res.predict(medical.iloc[train])\n",
    "        preds = res.predict(medical.iloc[test])\n",
    "        trainRes.append(roc_auc_score(medical.iloc[train].UCURNINS, predsTrain))\n",
    "        valRes.append(roc_auc_score(medical.iloc[test].UCURNINS, preds))\n",
    "        \n",
    "    print(\"Train AUC:\", round(np.mean(trainRes), 4), \"Valid AUC:\", round(np.mean(valRes), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Có vẻ như đối với hồi quy logistic, kết quả **rất ổn định** cho cả kiểm định 5-fold và 10-fold. Hơn nữa, nó **không quá khớp nhiều** (overfit), vì sự khác biệt về giá trị của chỉ số đo lường (metric) cho tập huấn luyện và tập kiểm định là **tối thiểu**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:37:31.052685Z",
     "start_time": "2021-03-24T16:37:27.263403Z"
    }
   },
   "outputs": [],
   "source": [
    "predList = []\n",
    "indList = []\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state = random.randint(0, 10000))\n",
    "\n",
    "for train, test in kf.split(medical.index.values) :\n",
    "    \n",
    "    mod = sm.GLM.from_formula(formula = 'UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + ' + \n",
    "                          'UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + ' + \n",
    "                          'UEDUC3 + GENDER',\n",
    "                          data = medical.iloc[train],\n",
    "                          family = sm.families.Binomial()) \n",
    "    res = mod.fit()\n",
    "    predsTrain = res.predict(medical.iloc[train])\n",
    "    preds = res.predict(medical.iloc[test])\n",
    "    \n",
    "    predList.append(preds.tolist())\n",
    "    indList.append(medical.iloc[test].index.tolist())\n",
    "    \n",
    "    trainRes.append(roc_auc_score(medical.iloc[train].UCURNINS, predsTrain))\n",
    "    valRes.append(roc_auc_score(medical.iloc[test].UCURNINS, preds))\n",
    "    \n",
    "print(\"Train AUC:\", round(np.mean(trainRes), 4), \"Valid AUC:\", round(np.mean(valRes), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:39:05.139258Z",
     "start_time": "2021-03-24T16:39:05.111405Z"
    }
   },
   "outputs": [],
   "source": [
    "predsSorted = pd.Series(sum(predList, []), index = sum(indList, [])).sort_index()\n",
    "roc_auc_score(medical.UCURNINS.sort_index(), pd.Series(sum(predList, []), index = sum(indList, [])).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:40:29.997849Z",
     "start_time": "2021-03-24T16:40:26.239403Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "predList = []\n",
    "indList = []\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = random.randint(0, 10000))\n",
    "\n",
    "for train, test in kf.split(medical.index.values, medical.UCURNINS) :\n",
    "    \n",
    "    mod = sm.GLM.from_formula(formula = 'UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + ' + \n",
    "                          'UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + ' + \n",
    "                          'UEDUC3 + GENDER',\n",
    "                          data = medical.iloc[train],\n",
    "                          family = sm.families.Binomial())\n",
    "    res = mod.fit()\n",
    "    predsTrain = res.predict(medical.iloc[train])\n",
    "    preds = res.predict(medical.iloc[test])\n",
    "    \n",
    "    predList.append(preds.tolist())\n",
    "    indList.append(medical.iloc[test].index.tolist())\n",
    "    \n",
    "    trainRes.append(roc_auc_score(medical.iloc[train].UCURNINS, predsTrain))\n",
    "    valRes.append(roc_auc_score(medical.iloc[test].UCURNINS, preds))\n",
    "    \n",
    "print(\"Train AUC:\", round(np.mean(trainRes), 4), \"Valid AUC:\", round(np.mean(valRes), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nhưng, đừng quên về chất lượng mô hình tổng thể...\n",
    "\n",
    "Phần lớn mọi người đều có bảo hiểm, chỉ bằng cách **đoán bừa** rằng ai đó có bảo hiểm, chúng ta đã đúng trong **85%** trường hợp, và chất lượng tổng thể của mô hình của chúng ta chỉ cao hơn một chút so với con số này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Overall percent of uninsured persons is %s' %round(medical.UCURNINS.sum() / medical.UCURNINS.count() * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Quality score for training set is %s' \n",
    "      %round(sum(medical.UCURNINS.iloc[train] == (predsTrain > 0.5) * 1) / len(train) * 100, 2))\n",
    "print(f'Quality score for test set is %s' \n",
    "      %round(sum(medical.UCURNINS.iloc[test] == (preds > 0.5) * 1) / len(test) * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#--------------------------------------------------------------------\n",
    "## Bài tập 2.\n",
    "\n",
    "### Bài tập 2.1.\n",
    "\n",
    "**Dữ liệu hành khách Titanic – 1310 quan sát và 15 biến:**\n",
    "* **passenger_id** – Id hành khách duy nhất\n",
    "* **pclass** – Hạng vé (1 = hạng 1, 2 = hạng 2, 3 = hạng 3)\n",
    "* **survived** – Sống sót (0 = Không, 1 = Có)\n",
    "* **name** – Tên và Họ\n",
    "* **sex** – Giới tính (0 = Nam, 1 = Nữ)\n",
    "* **age** – Tuổi tính bằng năm\n",
    "* **sibsp** – Số anh chị em ruột / vợ chồng trên tàu Titanic\n",
    "* **parch** – Số cha mẹ / con cái trên tàu Titanic\n",
    "* **ticket** – Số vé\n",
    "* **fare** – Giá vé hành khách\n",
    "* **cabin** – Số cabin\n",
    "* **embarked** – Cảng lên tàu (C = Cherbourg, Q = Queenstown, S = Southampton)\n",
    "* **boat** – Thuyền cứu sinh (nếu sống sót)\n",
    "* **body** – Số thi thể (nếu không sống sót và thi thể được tìm thấy)\n",
    "* **home.dest** – Quê quán/Điểm đến\n",
    "\n",
    "---\n",
    "\n",
    "**Yêu cầu:**\n",
    "\n",
    "1.  **Chạy lại các mô hình tốt nhất** của bạn cho tất cả các thuật toán với **5-fold CV**.\n",
    "2.  **Kiểm tra sự ổn định** của kết quả cho **K-fold lặp lại**.\n",
    "3.  Kiểm tra trong k-fold CV lặp lại xem việc thêm **phân tầng** có thay đổi kết quả của bạn không (sự ổn định).\n",
    "4.  Kiểm tra xem bạn có bị **quá khớp** trong các mô hình của mình không. Kiểm tra xem bạn có thể cải thiện điểm kiểm định (**validation score**) của mình không.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài tập 2.2.\n",
    "**Bộ Dữ liệu Chất lượng Rượu: \"data/wines.csv\"**\n",
    "* Nguồn: https://archive.ics.uci.edu/ml/datasets/wine+quality\n",
    "* Tệp chứa dữ liệu về các mẫu rượu vang trắng và đỏ Bồ Đào Nha **Vinho Verde**.\n",
    "* Các đặc điểm lý hóa khác nhau của từng mẫu có sẵn cũng như **điểm chất lượng rượu** trên thang điểm (0-10) được thực hiện bởi các chuyên gia.\n",
    "\n",
    "---\n",
    "\n",
    "**Yêu cầu:**\n",
    "\n",
    "1.  **Chạy lại các mô hình tốt nhất** của bạn cho tất cả các thuật toán với **5-fold CV**.\n",
    "2.  **Kiểm tra sự ổn định** của kết quả cho **K-fold lặp lại**.\n",
    "3.  Kiểm tra trong k-fold CV lặp lại xem việc thêm **phân tầng** có thay đổi kết quả của bạn không (sự ổn định).\n",
    "4.  **So sánh ảnh hưởng của phân tầng** với vấn đề Titanic.\n",
    "5.  Kiểm tra xem bạn có bị **quá khớp** trong các mô hình của mình không. Kiểm tra xem bạn có thể cải thiện điểm kiểm định (**validation score**) của mình không."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
